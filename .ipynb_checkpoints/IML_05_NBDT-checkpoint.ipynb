{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "## Practical Session 05\n",
    "\n",
    "Sharon Ong, Department of Cognitive Science and Artificial Intelligence – Tilburg University \n",
    "# Naive Bayes and Decision Trees "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Naive Bayes Classifers \n",
    "Naive Bayes classifiers are a family of classifiers that learn parameters by looking at each feature individually and collect simple per-class statistics from each feature.\n",
    "\n",
    "There are three kinds of naive Bayes classifiers implemented in scikit-learn\n",
    "* Gaussian Naive Bayes (_GaussianNB_)\n",
    "* Multinomial Naive Bayes (_MultinomialNB_)  \n",
    "* Bernoulli Naive Bayes (_BernoulliNB_) \n",
    "\n",
    "Gaussian Naive Bayes classifiers assume that features follow a normal distribution. Multinomial Naive Bayes classifiers assume count data (that is, that each feature represents an integer count of something, like how often a word appears in a sentence or a digit image). Bernoulli Naive Bayes models are useful if your feature vectors are binary (i.e. zeros and ones) or continuous values which can be precisely split (binarized) with a predeﬁned threshold . One application would be text classification with ‘bag of words’ model where the 1s & 0s are “word occurs in the document” and “word does not occur in the document” respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1  Gaussian Naive Bayes Classifer\n",
    "The following code implements a Gaussian Naive Bayes classifer to predict someone's gender from their height, weight and footsize.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False  True  True  True  True]\n",
      "[ True  True  True  True False False False False]\n",
      "[180.  177.6 167.4 177.6 150.  165.  162.6 172.5  90.   85.   83.   82.\n",
      "  50.   75.   65.   75.   12.   11.   12.   10.    6.    8.    7.    9. ]\n",
      "[[180.   90.   12. ]\n",
      " [177.6  85.   11. ]\n",
      " [167.4  83.   12. ]\n",
      " [177.6  82.   10. ]\n",
      " [150.   50.    6. ]\n",
      " [165.   75.    8. ]\n",
      " [162.6  65.    7. ]\n",
      " [172.5  75.    9. ]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pandas as pd \n",
    "\n",
    "gender = ['male','male','male','male','female','female','female','female']\n",
    "height = [180,177.6,167.4,177.6,150,165,162.6,172.5]\n",
    "weight = [90,85,83,82,50,75,65,75]\n",
    "foot_size = [12,11,12,10,6,8,7,9] \n",
    "\n",
    "# the following code convert the list \"gender\" to numerical data (0 for male and 1 for female) \n",
    "# to create the target variable\n",
    "b = pd.get_dummies(gender)\n",
    "\n",
    "print(b['female'].values)\n",
    "print(b['male'].values)\n",
    "y = b['female'].values\n",
    "\n",
    "# Create the feature vector \n",
    "X = np.concatenate((height, weight,foot_size), axis=0)\n",
    "# Feature vector is  \n",
    "print(X)\n",
    "X = X.reshape(len(height),3,order='F')\n",
    "print(X)\n",
    "\n",
    "# train a Naive Bayes classifier. \n",
    "clf = GaussianNB()\n",
    "clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new person has the following feature variables [Height = 180. Weight = 80, Foot Size = 10]. The following code predicts \n",
    "the class (gender) and the probability for each class.  \n",
    "\n",
    "**Exercise 1**\n",
    "\n",
    "Try varying the height, weight and foot size values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Your code goes here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "Apply a Gaussian Naive Bayes classifier on the dataset plot and display the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Apply this on the two moons dataset \n",
    "# and then see how it works \n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "#\n",
    "# Your code goes here \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Bernoulli Naive Bayes Classifer\n",
    "\n",
    "If X is random variable Bernoulli-distributed, it can assume only two values (for simplicity, let’s call them 0 and 1). \n",
    "\n",
    "The following code generates a dummy dataset. Bernoulli naive Bayes expects binary feature vectors. The class BernoulliNB has a binarize parameter which allows specifying a threshold that will be used internally to transform the features. \n",
    "\n",
    "By setting a threshold of 0.0, each point can be characterized by the quadrant where it’s located. All Feature 0 that are less or equal than 0.0 are set to 0 and all Feature 0 values are set to 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import mglearn\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nb_samples = 300\n",
    "[X, y] = make_classification(n_samples=nb_samples, n_features=2, n_informative=2, n_redundant=0)\n",
    "plt.figure()\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# train a Bernoulli Naive Bayes\n",
    "bnb = BernoulliNB(binarize=0.0)\n",
    "\n",
    "bnb.fit(X_train,y_train)\n",
    "print(bnb.score(X_test, y_test))\n",
    "# you can check the Bernoulli NB solution as well \n",
    "data = np.array([[0, -1], [1, 0], [-1, -1], [1, 1]])\n",
    "print(bnb.predict(data))\n",
    "print(bnb.predict_proba(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**\n",
    "\n",
    "Compare the Bernoulli naive Bayes with the Gaussian naive Bayes classifier on this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Your code goes here \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Multinomial Naive Bayes Classifer\n",
    "\n",
    "Let's compare the performance of a multinomial naive Bayes and Gaussian naive Bayes with the digit dataset. This dataset contains images of hand-written digits with 10 classes.  Each class refers to a digit. Each sample (belonging to 10 classes) is an 8×8 image encoded as an unsigned integer (0 – 255). The Multinomial classifer has been implemented for you.  \n",
    "\n",
    "**Exercise 4**\n",
    "\n",
    "Compare its performance with a Gaussian naive Bayes classifer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# let's extract out the 3rd row to the variable 'img' \n",
    "img = digits.data[2]\n",
    "plt.figure()\n",
    "plt.imshow(img.reshape(8,8)) \n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train,y_train)\n",
    "print(mnb.score(X_test, y_test))\n",
    "\n",
    "#\n",
    "# Your code goes here \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Decision Tree classifier \n",
    "\n",
    "Learning a decision tree involves learning the sequence of if/else questions that gets us to the \"true\" answer with the least number of questions. In the machine learning setting, these questions are called tests (not to be confused with the test set, which is the data we use to test to see how generalizable our model is). \n",
    "\n",
    "Usually data does not come in the form of binary yes/no features as in the animal example, but is instead represented as continuous features such as in the 2D dataset we will explore. The tests that are used on continuous data are of the form “Is feature i larger than value a?” To build a tree, the algorithm searches over all possible tests and finds the one that is most informative about the target variable.\n",
    "\n",
    "## 2.1 Decision Tree classifier on a 2D dataset.\n",
    "The following code implements a Decision Tree Classifer on a 2D dataset. The function get_depth() returns the depth of the tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import mglearn\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "plt.figure()\n",
    "mglearn.plots.plot_tree_partition(X_train, y_train, tree) \n",
    "print(tree.get_depth())\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can control the number of test(splits) performed by setting max_depth hyperparater. \n",
    "\n",
    "**Exercise 5**\n",
    "\n",
    "Change the max_depth to 1,2 and 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Your code goes here \n",
    "#\n",
    "tree = DecisionTreeClassifier(random_state=0, max_depth =1)\n",
    "tree.fit(X_train, y_train)\n",
    "plt.figure()\n",
    "mglearn.plots.plot_tree_partition(X_train, y_train, tree) \n",
    "print(tree.get_depth())\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\n",
    "\n",
    "#\n",
    "tree = DecisionTreeClassifier(random_state=0, max_depth =2)\n",
    "tree.fit(X_train, y_train)\n",
    "plt.figure()\n",
    "mglearn.plots.plot_tree_partition(X_train, y_train, tree) \n",
    "print(tree.get_depth())\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\n",
    "\n",
    "#\n",
    "tree = DecisionTreeClassifier(random_state=0, max_depth =4)\n",
    "tree.fit(X_train, y_train)\n",
    "plt.figure()\n",
    "mglearn.plots.plot_tree_partition(X_train, y_train, tree) \n",
    "print(tree.get_depth())\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You can also set the criterion for splitting. \n",
    " \n",
    " **Exercise 6**\n",
    " \n",
    " Change the criterion to \"entropy\" (criterion = \"entropy\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Your code goes here \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Decision Tree classifier on a high dimensional dataset \n",
    "Let’s look at the effect of max_depth in more detail on the Breast Cancer dataset. As always, we import the dataset and split it into a training and a test part. Then we build a model using the default setting of fully developing the tree (growing the tree until all leaves are pure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\n",
    "\n",
    "#display the tree depth \n",
    "print(tree.get_depth())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7**\n",
    "\n",
    "Set max_depth to 3,4,5 or 6. Which gives you max_depth returns the best test accuarcy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Your code goes here \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Decision Tree Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees can also be applied for regression in a Decision TreeRegressor. The code below implements a Decision Tree Regressor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# #############################################################################\n",
    "# Generate sample data\n",
    "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "\n",
    "# #############################################################################\n",
    "# Add noise to targets\n",
    "y[::5] += 3 * (0.5 - np.random.rand(8))\n",
    "\n",
    "regr = DecisionTreeRegressor()\n",
    "regr.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "y_pred = regr.predict(X_test)\n",
    "print(regr.get_depth())\n",
    "plt.figure()\n",
    "plt.scatter(X,y)\n",
    "plt.plot(X_test,y_pred, 'r', label='max_depth = 8')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8**\n",
    "\n",
    "Vary the max_depth parameter from 1,2,3 to 4. Plot the results of your regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Your code goes here \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one particular property of using tree-based models for regression is that they are not able to extrapolate, or make predictions outside of the range of the training data. We will use the dataset in \"ram_price.csv\" which is the historical computer memory (RAM) prices. The training data is the historical data up to the year 2000. The test data is the RAM prices after the year 2000.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ram_prices = pd.read_csv(\"ram_price.csv\")\n",
    "\n",
    "data_train = ram_prices[ram_prices.date < 2000]\n",
    "data_test = ram_prices[ram_prices.date >= 2000]\n",
    "# predict prices based on date\n",
    "X_train = data_train.date[:, np.newaxis]\n",
    "# we use a log-transform to get a simpler relationship of data to target\n",
    "y_train = np.log(data_train.price)\n",
    "\n",
    "X_test = data_test.date[:, np.newaxis] #ram_prices.date[:, np.newaxis] # data_test.date[:, np.newaxis]\n",
    "# we use a log-transform to get a simpler relationship of data to target\n",
    "y_test = np.log(data_test.price) # np.log(ram_prices.price) #\n",
    "\n",
    "plt.semilogy(X_train,np.exp(y_train),label='train')\n",
    "plt.semilogy(X_test,np.exp(y_test),label='test')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Price in $/Mbyte\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9**\n",
    "\n",
    "Train a Linear Regression and a Decision Tree Regression. Make the predictions over the test data and visualize your solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Your code goes here \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Naive Bayes Classifier without sklearn libraries \n",
    "Let's create a Naive Bayes Classifier without sklearn libraries to predict someone's gender from their height, weight and footsize. The following creates a pandas dataframe with the data we previously used in this worksheet.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe\n",
    "data = pd.DataFrame()\n",
    "\n",
    "# Create the target variable (y)\n",
    "data['Gender'] = ['male','male','male','male','female','female','female','female']\n",
    "\n",
    "# Create the feature variables (X)\n",
    "data['Height'] = [180,177.6,167.4,177.6,150,165,162.6,172.5]\n",
    "data['Weight'] = [90,85,83,82,50,75,65,75]\n",
    "data['Foot_Size'] = [12,11,12,10,6,8,7,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Bayes classifier, we are interested in finding out the class (e.g. male or female) of an observation given the data:\n",
    "\n",
    "\\begin{equation}    \n",
    "p(\\text{class} \\mid \\mathbf {\\text{data}} )={\\frac {p(\\mathbf {\\text{data}} \\mid \\text{class}) * p(\\text{class})}{p(\\mathbf {\\text{data}} )}}\n",
    "\\end{equation}\n",
    "where\n",
    "- $\\text{class}$ is a particular class (e.g. male)\n",
    "- $\\mathbf {\\text{data}}$ is an observation’s data\n",
    "- $p(\\text{class} \\mid \\mathbf{\\text{data}})$ is called the posterior\n",
    "- $p(\\text{data|class})$ is the likelihood\n",
    "- $p(\\text{class})$ is called the prior\n",
    "- $p(\\text{data})$ is called the marginal probability\n",
    "\n",
    "In a Bayes classifier, we calculate the posterior for every class for each observation. Then, classify the observation based on the class with the largest posterior value. In our example, we have one observation to predict and two possible classes (e.g. male and female), therefore we will calculate two posteriors: one for male and one for female.\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "p(\\text{person is male} \\mid \\mathbf {\\text{person’s data}} ) &={\\frac {p(\\mathbf {\\text{person’s data}} \\mid \\text{person is male}) * p(\\text{person is male})}{p(\\mathbf {\\text{person’s data}} )}}  \\\\\n",
    "&= {\\frac {p({\\text{male}})\\,p({\\text{height}}\\mid{\\text{male}})\\,p({\\text{weight}}\\mid{\\text{male}})\\,p({\\text{foot size}}\\mid{\\text{male}})}{\\text{marginal probability}}}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "p(\\text{person is female} \\mid \\mathbf {\\text{person’s data}} ) &={\\frac {p(\\mathbf {\\text{person’s data}} \\mid \\text{person is female}) * p(\\text{person is female})}{p(\\mathbf {\\text{person’s data}} )}} \\\\\n",
    "&={\\frac {p({\\text{female}})\\,p({\\text{height}}\\mid{\\text{female}})\\,p({\\text{weight}}\\mid{\\text{female}})\\,p({\\text{foot size}}\\mid{\\text{female}})}{\\text{marginal probability}}}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "For the first equation:\n",
    "- $p(\\text{male})$ is the prior, which is the probability an observation is male.\n",
    "- $p({\\text{height}}\\mid{\\text{female}})\\,p({\\text{weight}}\\mid{\\text{female}})\\,p({\\text{foot size}}\\mid{\\text{female}})$ is the likelihood. \n",
    "- marginal probability is hard to compute, can be ignored for classification as the value is the same for all classes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the data in pandas.  The dataset above is used to construct our classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes rule \n",
    "\n",
    "## 4.1 Calculate Priors\n",
    "\n",
    "Priors are either constants or probability distributions. In this example, the priors are the probability of the target variable, which is the gender.  \n",
    "\n",
    "The following code counts the number of males, females and the total number of rows.\n",
    "\n",
    "**Exercise 10**\n",
    "\n",
    "(1) Next, find the priors by dividing the number of males or females over the total number of rows. In a bayes classifier, we are interested in finding out the class (e.g. male or female, spam or ham) of an observation given the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of males\n",
    "\n",
    "n_male = data['Gender'][data['Gender'] == 'male'].count()\n",
    "\n",
    "# Number of males\n",
    "n_female = data['Gender'][data['Gender'] == 'female'].count()\n",
    "\n",
    "# Total rows\n",
    "total_ppl = data['Gender'].count()\n",
    "\n",
    "#\n",
    "# Your code goes here \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Calculate Likelihood \n",
    "The following code finds the mean for each gender. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by gender and calculate the means of each feature\n",
    "data_means = data.groupby('Gender').mean()\n",
    "\n",
    "# View the values\n",
    "data_means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 11**\n",
    "\n",
    "(2) Find the variance of each feature for each gender using the var() command in pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Your code goes here \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is a function that you can call to calculate the likelihood of x given y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can calculate p(x|y) with the following function. Here x is the feature value of a new data point \n",
    "\n",
    "def p_x_given_y(x, mean_y, variance_y):\n",
    "\n",
    "    # Input the arguments into a probability density function\n",
    "    p = 1/(np.sqrt(2*np.pi*variance_y)) * np.exp((-(x-mean_y)**2)/(2*variance_y))\n",
    "    \n",
    "    # return p\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows you how to obtain the mean height, weight and footsize of the \"male\" class.  Find the mean for each feature for the female class. \n",
    "\n",
    "**Exercise 12**\n",
    "\n",
    "Find the variance of each feature for both classes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Means for male\n",
    "#print( data_means['Height'][data_means.index == 'male'].values[0])\n",
    "\n",
    "\n",
    "mhm = data_means['Height'][data_means.index == 'male'].values[0]\n",
    "mwm = data_means['Weight'][data_means.index == 'male'].values[0]\n",
    "mfm = data_means['Foot_Size'][data_means.index == 'male'].values[0]\n",
    "#\n",
    "# Your code goes here \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Apply Bayes Classifier To New Test Point\n",
    "In Gaussian Naive Bayes Classifiers, we assume that each feature is uncorrelated from each other. For example, that foot size is independent of weight or height etc.. This is obviously not true, and is a “naive” assumption - hence the name “naive Bayes.\" We also assume that the value of the features (e.g. the height, weight, etc..) are normally (Gaussian) distributed. This means that the likelihood can be evaluated using the following example equation: \n",
    "\n",
    "\\begin{equation}\n",
    "p(\\text{height}\\mid\\text{female})=\\frac{1}{\\sqrt{2\\pi\\text{variance of female height in the data}}}\\,e^{ -\\frac{(\\text{observation’s height}-\\text{average height of females in the data})^2}{2\\text{variance of female height in the data}} }\n",
    "\\end{equation} \n",
    "\n",
    "**Exercise 13**\n",
    "\n",
    "(3) A new person has the following feature variables. [Height = 170. Weight = 65, Foot Size = 10]. Evaluate the posterior for each feature. Predict whether this person is more likely to be male or female? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Your code goes here \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
