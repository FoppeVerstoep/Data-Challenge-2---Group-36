{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOzj0IwoJ4Od"
   },
   "source": [
    "# Introduction to Machine Learning\n",
    "# Practical Session 01\n",
    "\n",
    "Sharon Ong, Department of Cognitive Science and Artificial Intelligence – Tilburg University "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DShXpPXsJ4Oj"
   },
   "source": [
    "In this worksheet, we will introduce two datasets for classification; the Iris Species Dataset and the Handwritten Digit Dataset \n",
    "\n",
    "1. The IRIS Data\n",
    "2. The Handwritten Digit Classification \n",
    "\n",
    "In this worksheet, we will also work with \n",
    "\n",
    "3. Categorcal data\n",
    "4. Text Data \n",
    "5. Text classification on the 20 newsgroups dataset \n",
    "6. Building Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The Iris Species Dataset\n",
    "In the 1920's, botanists collected measurements on the sepal length and width, and on the petal length and width of 150 irises, 50 from each of three species (setosa, versicolor, virginica) The measurements became known as Fisher's iris data. \n",
    "\n",
    "The question: is it possible to define the species of an iris based on these four measurements? Let's meet the data.  \n",
    "(use the tutorials for better way of doing things or using the slides) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris object that is returned by load_iris is a Bunch object, which is very similar\n",
    "to a dictionary. It contains keys and value. To display the keys: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of iris_dataset:\n",
      " dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Keys of iris_dataset:\\n\", iris_dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the key DESCR is a short description of the dataset. To display the beginning of the description, try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Informati\n",
      "...\n",
      "Target names: ['setosa' 'versicolor' 'virginica']\n",
      "Feature names:\n",
      " ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "print(iris_dataset['DESCR'][:250] + \"\\n...\")\n",
    "print(\"Target names:\", iris_dataset['target_names'])\n",
    "print(\"Feature names:\\n\", iris_dataset['feature_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris object that is returned by load_iris is a Bunch object, which is very similar\n",
    "to a dictionary. It contains keys and values. To learn more of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of data: <class 'numpy.ndarray'>\n",
      "Shape of data: (150, 4)\n",
      "First five rows of data:\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "Type of target: <class 'numpy.ndarray'>\n",
      "Shape of target: (150,)\n",
      "Target:\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of data:\", type(iris_dataset['data']))\n",
    "print(\"Shape of data:\", iris_dataset['data'].shape)\n",
    "print(\"First five rows of data:\\n\", iris_dataset['data'][0:5])\n",
    "print(\"Type of target:\", type(iris_dataset['target']))\n",
    "print(\"Shape of target:\", iris_dataset['target'].shape)\n",
    "print(\"Target:\\n\", iris_dataset['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Using numpy, create three data arrays; setosa, versicolor and virginica. Each numpy array is specific for that iris species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5]\n",
      " [4.9 3. ]\n",
      " [4.7 3.2]\n",
      " [4.6 3.1]\n",
      " [5.  3.6]\n",
      " [5.4 3.9]\n",
      " [4.6 3.4]\n",
      " [5.  3.4]\n",
      " [4.4 2.9]\n",
      " [4.9 3.1]\n",
      " [5.4 3.7]\n",
      " [4.8 3.4]\n",
      " [4.8 3. ]\n",
      " [4.3 3. ]\n",
      " [5.8 4. ]\n",
      " [5.7 4.4]\n",
      " [5.4 3.9]\n",
      " [5.1 3.5]\n",
      " [5.7 3.8]\n",
      " [5.1 3.8]\n",
      " [5.4 3.4]\n",
      " [5.1 3.7]\n",
      " [4.6 3.6]\n",
      " [5.1 3.3]\n",
      " [4.8 3.4]\n",
      " [5.  3. ]\n",
      " [5.  3.4]\n",
      " [5.2 3.5]\n",
      " [5.2 3.4]\n",
      " [4.7 3.2]\n",
      " [4.8 3.1]\n",
      " [5.4 3.4]\n",
      " [5.2 4.1]\n",
      " [5.5 4.2]\n",
      " [4.9 3.1]\n",
      " [5.  3.2]\n",
      " [5.5 3.5]\n",
      " [4.9 3.6]\n",
      " [4.4 3. ]\n",
      " [5.1 3.4]\n",
      " [5.  3.5]\n",
      " [4.5 2.3]\n",
      " [4.4 3.2]\n",
      " [5.  3.5]\n",
      " [5.1 3.8]\n",
      " [4.8 3. ]\n",
      " [5.1 3.8]\n",
      " [4.6 3.2]\n",
      " [5.3 3.7]\n",
      " [5.  3.3]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Your code goes here \n",
    "#\n",
    "import numpy as np\n",
    "setosa =  np.array(iris_dataset['data'][0:50])\n",
    "versicolor =  np.array(iris_dataset['data'][50:100])\n",
    "virginica = np.array(iris_dataset['data'][100:150])\n",
    "\n",
    "print(setosa[:,0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2) Compute the statistics (mean, standard deviation) for each measurement, for each iris species. Which two measurements have the highest correlation coefficient (for each species). Hint, use: corrcoef function from numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Your code goes here \n",
    "#\n",
    "print('setosa')\n",
    "setosa_mean = np.mean(setosa,axis=0)\n",
    "setosa_std = np.std(setosa,axis=0)\n",
    "print(setosa_mean, setosa_std)\n",
    "\n",
    "print(np.corrcoef(setosa,rowvar=False))\n",
    "print('versicolor')\n",
    "setosa_mean = np.mean(vers,axis=0)\n",
    "setosa_std = np.std(setosa,axis=0)\n",
    "print(setosa_mean, setosa_std)\n",
    "\n",
    "print(np.corrcoef(setosa,rowvar=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Generate six plots, each showing the values of two characterisitics of the irises, for all three types of iris. Label the x and y axis of each plot. \n",
    "The six plots are:\n",
    "1. sepal length vs sepal width\n",
    "2. sepal length vs petal length\n",
    "3. sepal length vs petal width\n",
    "4. sepal width vs petal length\n",
    "5. sepal width vs petal width\n",
    "6. petal length vs petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Your code goes here \n",
    "# show how you can ..\n",
    "# \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "#1\n",
    "plt.figure()\n",
    "plt.plot(setosa[:,0],setosa[:,1],'rs')\n",
    "plt.plot(versicolor[:,0],versicolor[:,1],'ko')\n",
    "\n",
    "#2\n",
    "plt.figure()\n",
    "plt.plot(setosa[:,0],setosa[:,2],'rd')\n",
    "plt.plot(versicolor[:,0],versicolor[:,2],'g+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the scatter matrix tell us? From the individual plots, we can see that the three classes seem to be relatively well separated using the sepal and petal measurements. This means that a machine learning model will likely be able to learn to separate them. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRwR8-4sJ4Om",
    "tags": []
   },
   "source": [
    "# 2. Binary Image (Handwritten Digit) Classification \n",
    "\n",
    "The digits dataset for scikit-learn are grayscale images of handwritten digits of 8 pixels by 8 pixels. The dataset comprises of 10 classes one for each digit. The code below loads the digits dataset for scikit-learn, displays some of the images and splits the dataset into training and testing \n",
    "The `imshow` functifrom sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()on can be used to display a binary, gray or color image. If the image is binary or grayscale, it is a 2D array. Color images are 3D arrays where the 3rd dimension is the color channel.   \n",
    "For binary and grayscale images, `cmap` parameter transforms the pixel intensities to a colormap. For information on the different colormaps can be found here.  https://matplotlib.org/3.1.0/gallery/color/colormap_reference.html\n",
    "To change the default colormap to a grayscale ranging from white and black, you set `cmap=plt.cm.gray`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "n3VQofXxJ4On",
    "outputId": "eda9dfbb-b749-42c8-cf60-eec34bbfb4d8"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load the digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# displays one image from each class \n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5),\n",
    "subplot_kw={'xticks':(), 'yticks': ()})\n",
    "for ax, img in zip(axes.ravel(), digits.images):\n",
    "    ax.imshow(img, cmap=plt.cm.gray)   \n",
    "\n",
    "# splits the data into training and testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, random_state=42)    \n",
    "print(X_train.shape)\n",
    "# Show that you can use the data to do..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2Obc8lnJ4Op"
   },
   "source": [
    "The code belows run a Dummy Classifier and display the accuracy and confusion matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"uniform\")\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "print(dummy_clf.score(X_train,y_train))\n",
    "print(dummy_clf.score(X_test,y_test))\n",
    "\n",
    "pred = dummy_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f2dmFw9J4Oq"
   },
   "source": [
    "The code belows show how you can display some of the misclassified images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "9WPgeQuZJ4Oq",
    "outputId": "1359c6b9-f157-4acd-a2d3-8d5757dadf61"
   },
   "outputs": [],
   "source": [
    "# append all the indexes of misclassified into an array \n",
    "index = 0\n",
    "misclassifiedIndexes = []\n",
    "for label, predict in zip(y_test, pred):\n",
    "    if label != predict: \n",
    "        misclassifiedIndexes.append(index)\n",
    "    index +=1\n",
    "\n",
    "# plot the corresponding image of the 6th to 9th element in the array \n",
    "plt.figure(figsize=(20,4))\n",
    "for plotIndex, badIndex in enumerate(misclassifiedIndexes[6:10]):\n",
    "    plt.subplot(1, 5, plotIndex + 1)\n",
    "    # reshape the image from a 64 by 1 vector to a 8 by 8 matrix \n",
    "    plt.imshow(np.reshape(X_test[badIndex], (8,8)), cmap=plt.cm.gray)    \n",
    "    plt.title('Predicted: {}, Actual: {}'.format(pred[badIndex], y_test[badIndex]), fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI77JhyZJ4Ox"
   },
   "source": [
    "# 3. Working with Categorical Data\n",
    "Almost all machine learning algoirthms are applied on numerical data. One way to convert categorical values to numerical values is one hot encoding. This creates a binary column for each category and returns a sparse matrix or dense array (depending on the sparse parameter). \n",
    "\n",
    "There are two ways to implement one hot encoding. Using the \"get_dummies\" function in a pandas library or the \"OneHotEncoder\" in sklearn.  Below is the dataset of categorical data which we have used before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "Doa39pbeJ4Ox"
   },
   "outputs": [],
   "source": [
    "X_train = {'Taste':['Salty','Spicy','Spicy','Spicy','Spicy','Sweet','Salty','Sweet','Spicy','Salty'], #Feature 1\n",
    "       'Temperature':['Hot','Hot','Hot','Cold','Hot','Cold','Cold','Hot','Cold','Hot'], #Feature 2\n",
    "       'Texture':['Soft','Soft','Hard','Hard','Hard','Soft','Soft','Soft','Soft','Hard']}  #Feature 3\n",
    "y_train = {'Eat':['No','No','Yes','No','Yes','Yes','No','Yes','Yes','Yes']}  #target variable \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlc04HJlJ4Ox"
   },
   "source": [
    "The following code converts the categorical data into numerical data with pandas  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "id": "sGbEuNhyJ4Oy",
    "outputId": "94283844-3d83-4f63-f771-b0bcfecd7d60"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load the data into a pandas dataframe \n",
    "data = pd.DataFrame(X_train)\n",
    "display(data)\n",
    "# convert catergorical data to numerical data with one hot encoding \n",
    "X_train_ohe  = pd.get_dummies(data)\n",
    "display(X_train_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8K0nh9dJ4Oy"
   },
   "source": [
    "The code below implements one hot encoding with the OneHotEncoder function in sklearn.  The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620
    },
    "id": "gXWYUOsXJ4Oy",
    "outputId": "c769bb51-ffe0-4a7c-a997-98b023f1d147"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "Taste = ['Salty','Spicy','Sweet'] #Feature 1\n",
    "Temperature =  ['Hot','Cold'] #Feature 2\n",
    "Texture = ['Soft','Hard']  #Feature 3\n",
    "\n",
    "# initialize a one hot encoder with specific categories \n",
    "enc = OneHotEncoder(categories=[Taste,Temperature,Texture]) \n",
    "print(enc)\n",
    "display(X_train.values())\n",
    "# X_train.values() extracts a list of variables from the dictionary y_train\n",
    "# Putting a * in front converts the y_trX_trainin.values() to an array of strings\n",
    "X_train_values = list(X_train.values())\n",
    "X_train_values_T = list(map(list, zip(*X_train_values)))\n",
    "display(X_train_values_T)\n",
    "\n",
    "# Input an array of strings (denoting the vlaues taken on by the categorical features)\n",
    "enc.fit(X_train_values_T)\n",
    "# \n",
    "display(pd.DataFrame(enc.transform(X_train_values_T).toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ohs0F8cJ4Oz"
   },
   "source": [
    "Below is code which converts the target variable (label) from categorical data to numerical data with the LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UspZ5KpuJ4Oz",
    "outputId": "797eaa7c-77ba-4f5a-f986-e6ddb125213a"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "print(y_train)\n",
    "# y_train.values() extracts a list of variables from the dictionary y_train\n",
    "# Putting a * in front converts the y_train.values() to an array of strings\n",
    "y_train_lb = lb.fit_transform(*y_train.values())\n",
    "print(y_train_lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SCk1YheJ4Oz"
   },
   "source": [
    "## 3.1 Working with Categorical Data on a Dataset of Adult Incomes \n",
    "In this worksheet, we will use the dataset of adult incomes in the United States, derived from the 1994 census database.  With this dataset, you can predict whether a worker has an income of over 50,000 dollars or under 50,000 dollars. The features in this dataset include the workers’ ages, how they are employed (self employed, private industry employee, government employee, etc.), their education, their gender, their working hours per week, occupation, and more. \n",
    "\n",
    "In this dataset, age and hours-per-week are continuous features, which we know how to treat. The workclass, education, sex, and occupation features are categorical, however. All of them come from a fixed list of possible values, as opposed to a range, and denote a qualitative property, as opposed to a quantity. Let's load and explore the dataset.  \n",
    "\n",
    "The dataset contains three files:\n",
    "* adult.data (the training dataset)\n",
    "* adult.names (dataset description)\n",
    "* adult.test (test dataset)\n",
    "\n",
    "Let's load and explore the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "QUB9ElosJ4O0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "train_data = pd.read_csv(\"adult.data\", header=None, index_col=False,names=['age', 'workclass', 'fnlwgt', 'education', 'education-num','marital-status', 'occupation', 'relationship', 'race', 'gender', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country','income'])\n",
    "# For illustration purposes, we only select some of the columns\n",
    "train_data = train_data[['age', 'workclass', 'education', 'gender', 'hours-per-week','occupation', 'income']]\n",
    "test_data = pd.read_csv(\"adult.test\", header=None, skiprows=1, index_col=False,names=['age', 'workclass', 'fnlwgt', 'education', 'education-num','marital-status', 'occupation', 'relationship', 'race', 'gender', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country','income'])\n",
    "test_data = test_data[['age', 'workclass', 'education', 'gender', 'hours-per-week','occupation', 'income']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7qGVC8GJ4O0"
   },
   "source": [
    "There are some whitespaces before and after the data values. To trim all the whitespaces we use the separator *,*. The test dataset has a weird first line, hence we skip the line using skiprows=1. The missing values in the dataset are indicated by ?\n",
    "To get more information about the training data, try "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "2rmkFHqFJ4O0",
    "outputId": "bbae4569-a929-411d-e867-e3550f015f2a"
   },
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsT1fxt5J4O0"
   },
   "source": [
    "Observations\n",
    "* There are 32561 samples in the training dataset\n",
    "* There are both categorical and numerical columns in the dataset. Similarly, for the test dataset\n",
    "* There are 16281 samples in the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHTYmCi7J4O1",
    "outputId": "541404a4-3504-4f53-e3f1-1980b09e296e"
   },
   "outputs": [],
   "source": [
    "# how to we find the number of samples, number of cappr\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFuXspMGJ4O1"
   },
   "source": [
    "## 3.2 Handling Numerical Columns\n",
    "We select the numerical columns using the _select_dtypes_ function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P433P916J4O1",
    "outputId": "a46850fd-ccd5-41d8-997d-9f5ffdaac6d8"
   },
   "outputs": [],
   "source": [
    "num_attributes = train_data.select_dtypes(include=['int64'])\n",
    "print(num_attributes.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "Xrkj9M_WJ4O1",
    "outputId": "cc5ab0fb-15da-4141-ad27-713c5b65fbd6"
   },
   "outputs": [],
   "source": [
    "num_attributes.hist(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpBNReIgJ4O1"
   },
   "source": [
    "## 3.3 Handling Categorical Columns\n",
    "Let's explore the categorial columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rZsUI7YpJ4O2",
    "outputId": "4eadac66-6c09-432b-8b30-d652549cfee5"
   },
   "outputs": [],
   "source": [
    "cat_attributes = train_data.select_dtypes(include=['object'])\n",
    "print(cat_attributes.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "5sJRKaz7J4O2",
    "outputId": "6284d085-4a1c-4b15-9e61-f636acef161f"
   },
   "outputs": [],
   "source": [
    "# We will use countplot from the seaborn package.\n",
    "import seaborn as sns\n",
    "sns.countplot(y='workclass', hue='income', data = cat_attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "pTnPEXaVJ4O2",
    "outputId": "8750e71a-597c-4e9c-d922-b74b6da04b95"
   },
   "outputs": [],
   "source": [
    "sns.countplot(y='occupation', hue='income', data = cat_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEG1Q2blJ4O3"
   },
   "source": [
    "The goal is to find whether a worker has an income of over 50,000 dollars or under 50,000 dollars. The features in this dataset include the workers’ ages,how they are employed. Let's separate out the income data from the train and test data. Assign the income data as our labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "ilEaL0sbJ4O3"
   },
   "outputs": [],
   "source": [
    "# First we make a copy of the dataset \n",
    "train_copy = train_data.copy()\n",
    "# set income to 0 if the value is less than 50K\n",
    "train_copy[\"income\"] = train_copy[\"income\"].apply(lambda x:0 if x==train_copy[\"income\"][0] else 1)\n",
    "# Create a training feature without income\n",
    "X_train = train_copy.drop('income', axis =1)\n",
    "# Assign the income data as our labels. \n",
    "y_train = train_copy['income']\n",
    "\n",
    "test_copy = test_data.copy()\n",
    "# set income to 0 if the value is less than 50K\n",
    "test_copy[\"income\"] = test_copy[\"income\"].apply(lambda x:0 if x==test_copy[\"income\"][0] else 1) \n",
    "X_test = test_copy.drop('income', axis =1)\n",
    "y_test = test_copy['income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqjBpBQpJ4O3"
   },
   "source": [
    "## 3.4 One Hot Encoding \n",
    "Perform one hot encoding to convert the categorical data (X_train and X_test) to numerical data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eg_1PWASJ4O3",
    "outputId": "756454a1-02cc-4c27-fbd6-0d1d61efcc97"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Your code goes here \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noiEc6fVJ4O3"
   },
   "source": [
    "Train a dummy classifier. Test the performance of this classifer on the test data by evaluating the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Your code goes here \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLXssxwGJ4O5"
   },
   "source": [
    "# 4. Supervised Text Classification\n",
    "Applications such as translating documents, sentiment analysis, information extraction, information retrieval and question answering systems deal with a huge amount of text classification. Transforming text into something an algorithm can digest is a complicated process. \n",
    "\n",
    "Most classifiers expect numerical feature vectors with a fixed size and therefore, cannot directly process text data in their original form. Raw text documents have variable length. Therefore, preprocessing of text data is required to convert text to more manageable representations. The mapping from textual data to real valued vectors is called feature extraction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBdaBf5UJ4O5"
   },
   "source": [
    "# 4.1 Feature Extraction of Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ws0ZL7M9J4O5"
   },
   "source": [
    "scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n",
    "* tokenizing strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n",
    "* counting the occurrences of tokens in each document.\n",
    "* normalizing and weighting with diminishing importance tokens that occur in the majority of samples / documents.\n",
    "\n",
    "One of the most simple but effective and commonly used ways to represent text for machine learning is using the bag-of-words representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jE14qUFNJ4O5"
   },
   "source": [
    "_CountVectorizer_ extracts text data to the bag-of-words representations. Let's compute the bag-of-words representations to the following dataset; a list of four text lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9-vMo6IVJ4O5",
    "outputId": "f03b09df-7f69-485c-af15-89b34e054c49"
   },
   "outputs": [],
   "source": [
    "text_list = [\n",
    "    'This is the first line.',\n",
    "    'This line is the second line.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the second line?',\n",
    "]\n",
    "print(text_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-tvtjS3J4O6"
   },
   "source": [
    "The following code imports and instantiates the CountVectorizer and fit it to our text data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTyfPnSJJ4O6",
    "outputId": "9bc64f35-324c-4cdb-c163-3bf14994eba8"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "# Learn the vocabulary dictionary and return term-document matrix.\n",
    "X_count_vect = vectorizer.fit_transform(text_list)\n",
    "print(vectorizer)\n",
    "# Display term document matrix which shows the number of samples for each term \n",
    "print(X_count_vect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMAkWIt7J4O6"
   },
   "source": [
    "Each term found by _CountVectorizer_ during the fit is assigned a unique integer index corresponding to a column in the resulting matrix. This interpretation of the columns can be retrieved as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3XsYmn_J4O7",
    "outputId": "6f449840-6497-4849-8e1e-108df59c755a"
   },
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "print(X_count_vect.toarray())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLLezNR8J4O7"
   },
   "source": [
    "The mapping from feature name to column index is stored in the vocabulary_ attribute of the vectorizer. To find where a feature name in stored (find its column index),\n",
    "Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKzK_tivJ4O7",
    "outputId": "e87c73f3-ba42-4ce7-f511-b811a0ac57fb"
   },
   "outputs": [],
   "source": [
    "print(vectorizer.vocabulary_.get('line'))\n",
    "print(vectorizer.vocabulary_.get('this'))\n",
    "\n",
    "# Words that were not seen in the training text list will be completely ignored in future calls to the transform method: \n",
    "\n",
    "print(vectorizer.vocabulary_.get('rabbit'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qP_J7iRtJ4O7"
   },
   "source": [
    "## 4.2 Stop words \n",
    "Stop words are words like “and”, “the”, “him”, which are presumed to be uninformative in representing the content of a text, and which may be removed to avoid them being construed as signal for prediction. To remove some stopwords from the text list, try\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6VDoGg6bJ4O8",
    "outputId": "e8f9336d-0765-44f9-fcc4-8b21dc3fbb3a"
   },
   "outputs": [],
   "source": [
    "# remove the following stopwords ['is','this','the']\n",
    "vectorizer = CountVectorizer(stop_words=['is','this','the'])\n",
    "X_count_vect_sw = vectorizer.fit_transform(text_list)\n",
    "print(vectorizer.get_stop_words())\n",
    "print(X_count_vect_sw.toarray())  \n",
    "print(vectorizer.vocabulary_.get('this'))\n",
    "\n",
    "\n",
    "# remove stop words from the built-in list by setting stop_words=\"english\" \n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_count_vect_sw = vectorizer.fit_transform(text_list)\n",
    "# stopwords in the built in list \n",
    "print(vectorizer.get_stop_words())\n",
    "print(X_count_vect_sw.toarray())  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfqM7ivIJ4O8"
   },
   "source": [
    "We can also cut back the text used for prediction by using tokens which appear in at least two documents (or at least three documents, and so on). A token that appears only in a single document is unlikely to appear in the test set and is therefore not helpful. We can set the minimum number of documents a token needs to appear in with the _min_df_ parameter. Set the min_df parameter to 2. What do you get? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uZikY9FMJ4O8",
    "outputId": "9bd58e1d-31d2-4f95-c86e-54dce03a9279"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=2)\n",
    "X_count_min_dif = vectorizer.fit_transform(text_list)\n",
    "print(\"X_train with min_df: {}\".format(repr(X_count_min_dif)))\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(vectorizer.get_stop_words())\n",
    "print(X_count_min_dif.toarray())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7T_q_L2J4O8"
   },
   "source": [
    "## 4.3 Rescaling the Data with tf–idf\n",
    "Instead of dropping features that are deemed unimportant, another approach is to rescale features by how informative we expect them to be. One of the most common ways to do this is using the term frequency–inverse document frequency (tf–idf) method. The intuition of this method is to give high weight to any term that appears often in a particular document, but not in many documents in the corpus. If a word appears often in a particular document, but not in very many documents, it is likely\n",
    "to be very descriptive of the content of that document. scikit-learn implements the tf–idf method in two classes: TfidfTransformer, which takes in the sparse matrix output produced by CountVectorizer and transforms it, and TfidfVectorizer, which takes in the text data and does both the bag-of-words feature extraction and the tf–idf transformation. \n",
    "Note that scikit learn applies TfidfVectorizer slightly differently:\n",
    "https://towardsdatascience.com/how-sklearns-tf-idf-is-different-from-the-standard-tf-idf-275fa582e73d\n",
    "Try:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SLsv660QJ4O9",
    "outputId": "8784f405-779b-4626-aaf4-d68189a35392"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "X_train_tf = TfidfTransformer().fit_transform(X_count_vect)\n",
    "print(X_train_tf.toarray())\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tf = vectorizer.fit_transform(text_list)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X_tf.toarray())  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmHjpTxtJ4O9"
   },
   "source": [
    "## 4.4 Bag-of-Words with More Than One Word (n-Grams)\n",
    "One of the main disadvantages of using a bag-of-words representation is that word order is completely discarded. Fortunately, there is a way of capturing context when using a bag-of-words representation, by not only considering the counts of single\n",
    "tokens, but also the counts of pairs or triplets of tokens that appear next to each other.\n",
    "Pairs of tokens are known as bigrams, triplets of tokens are known as trigrams, and more generally sequences of tokens are known as n-grams. We can change the range of tokens that are considered as features by changing the _ngram_range_ parameter of\n",
    "_CountVectorizer_ or _TfidfVectorizer_. Try the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ze3a67AxJ4O9",
    "outputId": "d4d565eb-d099-4b9f-e36d-8310e0ce4c18"
   },
   "outputs": [],
   "source": [
    "# The default is to create one feature per sequence of tokens that is at least one token long and at most one token long, \n",
    "# or in other words exactly one token long (single tokens are also called unigrams):\n",
    "cv1 = CountVectorizer(ngram_range=(1, 1)).fit(text_list)\n",
    "print(\"Vocabulary size: {}\".format(len(cv1.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv1.get_feature_names_out()))\n",
    "\n",
    "# To look only at bigrams (sequences of two tokens following each other), we can set ngram_range to (2, 2):\n",
    "cv2 = CountVectorizer(ngram_range=(2, 2)).fit(text_list)\n",
    "print(\"Vocabulary size: {}\".format(len(cv2.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv2.get_feature_names_out()))\n",
    "\n",
    "# To look at a combination of unigrams, bigrams and trigrams, try:\n",
    "cv3 = CountVectorizer(ngram_range=(1, 3)).fit(text_list)\n",
    "print(\"Vocabulary size: {}\".format(len(cv3.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv3.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EgTrX0dJ4O-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsD-EWj8J4O-",
    "tags": []
   },
   "source": [
    "# 5. Text classification on  the 20 newsgroups dataset \n",
    "Let's create a classifer on a larger dataset: the 20 newsgroups dataset.\n",
    "\n",
    "The dataset is called “Twenty Newsgroups”. Here is the official description, quoted from the website:\n",
    "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "IwbQvN8lJ4O-"
   },
   "outputs": [],
   "source": [
    "# In order to get faster execution times for this first example we will work on a partial dataset with only 4 categories \n",
    "# out of the 20 available in the dataset:\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "l60ByQOAJ4O-"
   },
   "outputs": [],
   "source": [
    "# We can now load the list of files matching those categories as follows:\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ytWsyDt7J4O-",
    "outputId": "7bc4f762-6f4e-44e6-8b23-652c96145dbe"
   },
   "outputs": [],
   "source": [
    "print(twenty_train.target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9xfcnFgIJ4O_",
    "outputId": "5f5ebedd-98a5-4026-91a7-6075c22c71dc"
   },
   "outputs": [],
   "source": [
    "# Let's find the size of the files \n",
    "len(twenty_train.data)\n",
    " \n",
    "len(twenty_train.filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nRd_7KC6J4O_",
    "outputId": "0e6fc440-1c94-46e1-e699-5a69b33e4755"
   },
   "outputs": [],
   "source": [
    "# Let’s print the first lines of the first loaded file:\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\n",
    "print(twenty_train.target_names[twenty_train.target[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Yw6wxzNJ4O_"
   },
   "source": [
    "# 5.1 Extracting features from text files\n",
    "\n",
    "1. Extract features from the text files (twenty_train.data) with the _CountVectorizer_. You should fit the \"twenty_train.data \" and return the term-document matrix.\n",
    "2. Find the number of times the word \"images\" occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_CYPJZbJ4O_",
    "outputId": "881a60f4-b527-4389-b934-023f8dd52168"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "#\n",
    "# Your code goes here. \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VwpUNhJJ4PA"
   },
   "source": [
    "## 5.2 From occurrences to frequencies\n",
    "Occurrence count is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.To avoid these potential discrepancies, we can to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3UHGBdjYJ4PA",
    "outputId": "013c4bfd-a112-4da8-8909-7211a7c0bc4e"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "print(X_train_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhiL-xPjJ4PA"
   },
   "source": [
    "In the above example-code, we firstly use the fit(..) method to fit our estimator to the data and secondly the transform(..) method to transform our count-matrix to a tf-idf representation. These two steps can be combined to achieve the same end result faster by skipping redundant processing. This is done through using the fit_transform(..) method as shown below, and as mentioned in the note in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuGTjiz4J4PA"
   },
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48UmBqvZJ4PA"
   },
   "source": [
    "## 5.3 Training a classifier\n",
    "Now that we have our features, we can train a classifier to try to predict the category of a post. Train a dummy classifer on the extracted features (X_train_tfidf) and the labels (twenty_train.target). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LB0aSSymJ4PA"
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "#\n",
    "# Your code goes here \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syzGoHdKJ4PB"
   },
   "source": [
    "## 6.4 Predicting the result of a new list \n",
    "Predict the result of the following new test data (docs_new). You will have to transform the text list (docs_new) to a new  . Call the output of your predicted result \"pred\". Call vectorizer.transform on the test data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0tnAR2IJ4PB"
   },
   "outputs": [],
   "source": [
    "docs_new = ['God is love', 'CPU is Central Processing Unit','New Drug receives FDA approval']\n",
    "\n",
    "\n",
    "#\n",
    "# Your code goes here \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slksvXvkJ4PB"
   },
   "source": [
    "The following code displays the predictions of the new data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bLUlMTzvJ4PB",
    "outputId": "efcb6607-c5a6-45f9-9f8c-857a1fa9ca55"
   },
   "outputs": [],
   "source": [
    "for doc, category in zip(docs_new, pred):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9Et4EI-J4PB"
   },
   "source": [
    "# 6. Building Pipelines for Text Classification  \n",
    "\n",
    "The _Pipeline_ class is a class that allows “gluing” together multiple processing steps into a single scikit-learn estimator.\n",
    ". The _Pipeline_ class has its own _fit_, _predict_, and _score_ methods and behaves just like any other model in scikit-learn. The code below applies a pipeline to make the vectorizer => transformer => classifier easier to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auXa2FIWJ4PC",
    "outputId": "8a985b44-1b93-4241-c9e8-62e1c3d16a34"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# create Pipeline\n",
    "text_clf1 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', DummyClassifier()),\n",
    "])\n",
    "\n",
    "#fit on training data \n",
    "text_clf1.fit(twenty_train.data, twenty_train.target)  \n",
    "\n",
    "#evaluate on test data\n",
    "twenty_test = fetch_20newsgroups(subset='test',categories=categories, shuffle=True, random_state=42)\n",
    "print(twenty_train.target_names)\n",
    "\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf1.predict(docs_test)\n",
    "print(np.mean(predicted == twenty_test.target))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
